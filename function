# ------------------------------------------------------------------------------
# Copyright (c) Microsoft
# Licensed under the MIT License.
# Written by Bin Xiao (Bin.Xiao@microsoft.com)
# ------------------------------------------------------------------------------
#-----------------------function_no_hr_feature
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
 
import time
import logging
import os

import numpy as np
import torch
import torch as nn

from config import cfg
from lib.core.evaluate import accuracy
from lib.core.inference import get_final_preds
from lib.utils.transforms import flip_back
from lib.utils.vis import save_debug_images
from skimage.metrics import structural_similarity as compare_ssim
from skimage.metrics import peak_signal_noise_ratio as compare_psnr
# from core.maximum_mean_discrepancy import JointsMSELoss
import matplotlib.pyplot as plt
import cv2
# from thop import profile, clever_format
from torchvision.utils import save_image
logger = logging.getLogger(__name__)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  ###如果没有gpu选择cpu
# batch_size=32
# # print('batch_size',batch_size)
# LABELS = torch.cat([torch.arange(batch_size) for i in range(1)], dim=0)
# LABELS = (LABELS.unsqueeze(0) == LABELS.unsqueeze(1)).float() #one-hot representations
# LABELS = LABELS.to(device)#([32, 32])
# print('LABELS ', LABELS.shape)
def ntxent_loss(features, features_1, temp=2):
    """
    NT-Xent Loss.

    Args:
    z1: The learned representations from first branch of projection head
    z2: The learned representations from second branch of projection head
    Returns:
    Loss
    """
    LABELS = torch.cat([torch.arange(features.shape[0]) for i in range(1)], dim=0)
    LABELS = (LABELS.unsqueeze(0) == LABELS.unsqueeze(1)).float()  # one-hot representations
    LABELS = LABELS.to(device)  # ([32, 32])
    similarity_matrix = torch.matmul(features_1, features.T)
    # print('similarity_matrix ', similarity_matrix .shape)
    mask = torch.eye(LABELS.shape[0], dtype=torch.bool).to(device)
    # print('mask',mask.shape)
    labels = LABELS[~mask].view(LABELS.shape[0], -1)
    similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)

    positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)

    negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)

    logits = torch.cat([positives, negatives], dim=1)
    labels = torch.zeros(logits.shape[0], dtype=torch.long).to(device)

    logits = logits / temp
    return logits, labels
# def guassian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):
#     n_samples = int(source.size()[0])+int(target.size()[0])
#     total = torch.cat([source, target], dim=0)
#     total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))
#     total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))
#     L2_distance = ((total0-total1)**2).sum(2)
#     if fix_sigma:
#         bandwidth = fix_sigma
#     else:
#         bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)
#     bandwidth /= kernel_mul ** (kernel_num // 2)
#     bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]
#     kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]
#     return sum(kernel_val)#/len(kernel_val)
# def mmd_rbf_noaccelerate(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):
#     batch_size = int(source.size()[0])
#     kernels = guassian_kernel(source, target,
#                               kernel_mul=kernel_mul, kernel_num=kernel_num, fix_sigma=fix_sigma)
#     XX = kernels[:batch_size, :batch_size]
#     YY = kernels[batch_size:, batch_size:]
#     XY = kernels[:batch_size, batch_size:]
#     YX = kernels[batch_size:, :batch_size]
#     loss = torch.mean(XX + YY - XY -YX)
#     return loss
sr_criterion = torch.nn.L1Loss().cuda()
criterion1 = torch.nn.CrossEntropyLoss()

import time
import logging
import os

import numpy as np
import torch

from core.evaluate import accuracy
from core.inference import get_final_preds
from utils.transforms import flip_back
from utils.vis import save_debug_images

logger = logging.getLogger(__name__)


def train(config, train_loader, model, criterion, optimizer, epoch,
          output_dir, tb_log_dir, writer_dict):
    batch_time = AverageMeter()
    data_time = AverageMeter()
    losses = AverageMeter()
    acc = AverageMeter()

    # switch to train mode
    model.train()

    end = time.time()
    for i, (input,lr_input, target, target_weight, meta) in enumerate(train_loader):
        # measure data loading time
        data_time.update(time.time() - end)
        # print('target============1', target.shape)
        # print('input', input.shape)
        # print('target_weight ==============1', target_weight)
        # compute output
        input = input.cuda(non_blocking=True)
        lr_input = lr_input.cuda(non_blocking=True)
        sr,outputs,hr_results= model(input,lr_input)
        # print('sr',sr.shape)
        from torchvision.utils import save_image
        save_image(sr, '/media/zou/D/low-resolution/mpii_8_sr+res50output/mpii/pose_resnet/256x256_d256x3_adam_lr1e-3/clean.png')
        # print('outputs',outputs.shape)
        # print('lr_input',lr_input.shape)
        # print('hr_feature', hr_feature.shape)

        target = target.cuda(non_blocking=True)
        # print('target ', target.shape)
        target_weight = target_weight.cuda(non_blocking=True)

        if isinstance(outputs, list):
            loss = criterion(outputs[0], target, target_weight)
            for output in outputs[1:]:
                loss += criterion(output, target, target_weight)
        else:
            output = outputs
            loss_pose = criterion(output, target, target_weight)
            # print('loss_pose',loss_pose)
            loss1 = criterion(hr_results, target, target_weight)
            loss_sr = sr_criterion(sr,input)
            # logits, labels = ntxent_loss(sr_feature,hr_feature)
            # loss_feature = mmd_rbf_noaccelerate(sr_feature, hr_feature)+0.001
            # print('loss_feature', loss_feature)
            loss = 0.9*(loss_pose + loss1) + 0.1*loss_sr #+ 0.01*criterion1(logits,labels)

        # loss = criterion(output, target, target_weight)
        # compute gradient and do update step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # measure accuracy and record loss
        losses.update(loss.item(), input.size(0))

        _, avg_acc, cnt, pred = accuracy(output.detach().cpu().numpy(),
                                         target.detach().cpu().numpy())
        acc.update(avg_acc, cnt)

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        if i % config.PRINT_FREQ == 0:
            msg = 'Epoch: [{0}][{1}/{2}]\t' \
                  'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\t' \
                  'Speed {speed:.1f} samples/s\t' \
                  'Data {data_time.val:.3f}s ({data_time.avg:.3f}s)\t' \
                  'Loss {loss.val:.5f} ({loss.avg:.5f})\t' \
                  'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(
                epoch, i, len(train_loader), batch_time=batch_time,
                speed=input.size(0) / batch_time.val,
                data_time=data_time, loss=losses, acc=acc)
            logger.info(msg)

            writer = writer_dict['writer']
            global_steps = writer_dict['train_global_steps']
            writer.add_scalar('train_loss', losses.val, global_steps)
            writer.add_scalar('train_acc', acc.val, global_steps)
            writer_dict['train_global_steps'] = global_steps + 1

            prefix = '{}_{}'.format(os.path.join(output_dir, 'train'), i)
            save_debug_images(config, input, meta, target, pred * 4, output,
                              prefix)


def denor(tensor,mean,std):
    for t,m,s in zip(tensor,mean,std):
        t.mul_(s).add(m)
    return tensor

def validate(config, val_loader, val_dataset, model, criterion, output_dir,
             tb_log_dir, writer_dict=None):
    batch_time = AverageMeter()
    losses = AverageMeter()
    acc = AverageMeter()

    # switch to evaluate mode
    model.eval()

    num_samples = len(val_dataset)  # 2958
    all_preds = np.zeros(
        (num_samples, config.MODEL.NUM_JOINTS, 3),
        dtype=np.float32
    )
    all_boxes = np.zeros((num_samples, 6))
    image_path = []
    filenames = []
    imgnums = []
    idx = 0
    with torch.no_grad():
        all_PSNR = 0
        all_SSIM = 0
        end = time.time()
        for i, (input, lr_input,target, target_weight, meta) in enumerate(val_loader):
            # compute output
            print('image_file', meta['image'])
            from torchvision import transforms
            # inputsr=input.cuda()
            sr,outputs,_, = model(input,lr_input)  # <class 'torch.Tensor'> torch.Size([64, 16, 64, 64])

            from torchvision.utils import save_image
            mean = [0.485, 0.456, 0.406]
            std = [0.229, 0.224, 0.225]

            MEAN = [-mean / std for mean, std in zip(mean, std)]
            STD = [1 / std for std in std]
            denormalizer = transforms.Normalize(mean=MEAN, std=STD)
            # 反归一化得到原图
            image20 = denormalizer(input)
            # print('imge20',image20.shape)
            # img2=image20[0][[2, 1, 0], :, :].permute(1, 2, 0)
            img2 = image20[0][:, :, :].permute(1, 2, 0)
            # plt.imshow(img2)
            # plt.axis('off')
            # plt.show()
            sr20=sr.cpu()
            sr2 = denormalizer(sr20)
            # sr20 = sr2[0][[2, 1, 0], :, :].permute(1, 2, 0)
            sr20 = sr2[0][:, :, :].permute(1, 2, 0)
            # plt.imshow(sr20)
            # plt.axis('off')
            # plt.show()
            fig,ax = plt.subplots(1,2)
            ax[0].imshow(sr20)
            # plt.axis('off')
            # plt.show()
            ax[1].imshow(img2)
            # plt.axis('off')
            plt.savefig('/media/zou/D/low-resolution/visual/mpii_hrnet/2/2.png')
            plt.savefig('/media/zou/D/low-resolution/mpii_8_sr+res50output/mpii/pose_resnet/256x256_d256x3_adam_lr1e-3/clean.png')
            plt.show()

            srHR = sr2[:,[2,1,0],:,:].numpy().squeeze(0)
            batchHR = image20[:,[2,1,0],:,:].numpy().squeeze(0)
            # srHR = sr2[:,:,:,:].numpy().squeeze(0)
            # batchHR = image20[:,:,:,:].numpy().squeeze(0)
            single_PSNR = compare_psnr(batchHR.T, srHR.T,data_range=255) #.astype('uint8')
            all_PSNR = single_PSNR + all_PSNR
            all_SSIM = compare_ssim(batchHR.T, srHR.T,channel_axis=2,data_range=255) + all_SSIM
            print('\r', 'ave_PSNR:{:.6f}  all_SSIM:{:.6f}'.format(all_PSNR / (i + 1), all_SSIM / (i + 1)), end='')

            # save_image(inputsr[:,[1,0,2],:,:],'/media/zou/D/low-resolution/mpii_8_sr+res50output/mpii/pose_resnet/256x256_d256x3_adam_lr1e-3/tu.png')
            # save_image(sr[:,[1,0,2],:,:],'/media/zou/D/low-resolution/mpii_8_sr+res50output/mpii/pose_resnet/256x256_d256x3_adam_lr1e-3/clean.png')
            if isinstance(outputs, list):
                output = outputs[-1]  # 只输出最后一个
            else:
                output = outputs

            if config.TEST.FLIP_TEST:
                # this part is ugly, because pytorch has not supported negative index
                input_flipped = np.flip(input.cpu().numpy(), 3).copy()
                input_flipped = torch.from_numpy(input_flipped).cuda()  # torch.Size([64, 3, 256, 256])
                lr_input_flipped = np.flip(lr_input.cpu().numpy(), 3).copy()
                lr_input_flipped = torch.from_numpy(lr_input_flipped).cuda()  # torch.Size([64, 3, 256, 256])
                _,outputs_flipped,_= model(input_flipped, lr_input_flipped)  # torch.Size([64, 16, 64, 64])

                if isinstance(outputs_flipped, list):
                    output_flipped = outputs_flipped[-1]
                else:
                    output_flipped = outputs_flipped

                output_flipped = flip_back(output_flipped.cpu().numpy(),
                                           val_dataset.flip_pairs)  # 将翻转过的输入变成正常的输出
                output_flipped = torch.from_numpy(output_flipped.copy()).cuda()

                # feature is not aligned, shift flipped heatmap for higher accuracy
                # 【】为啥翻转的没对齐
                if config.TEST.SHIFT_HEATMAP:
                    output_flipped[:, :, :, 1:] = \
                        output_flipped.clone()[:, :, :, 0:-1]  # 【c】将0以后的图左移动

                output = (output + output_flipped) * 0.5  # 【see】妙啊

            target = target.cuda(non_blocking=True)
            target_weight = target_weight.cuda(non_blocking=True)  # target_weight是否可见

            loss = criterion(output, target, target_weight)
            # criterion(output, target, target_weight) #【see】

            num_images = input.size(0)  # 求平均值用
            # measure accuracy and record loss
            losses.update(loss.item(), num_images)
            _, avg_acc, cnt, pred = accuracy(output.cpu().numpy(),
                                             target.cpu().numpy())

            acc.update(avg_acc, cnt)

            # measure elapsed time
            batch_time.update(time.time() - end)
            end = time.time()

            c = meta['center'].numpy()
            s = meta['scale'].numpy()  # meta只获取当前loader的 64个
            score = meta['score'].numpy()

            preds, maxvals = get_final_preds(
                config, output.clone().cpu().numpy(), c, s)

            all_preds[idx:idx + num_images, :, 0:2] = preds[:, :, 0:2]
            all_preds[idx:idx + num_images, :, 2:3] = maxvals  # (2958, 16, 3)
            # double check this all_boxes parts
            all_boxes[idx:idx + num_images, 0:2] = c[:, 0:2]
            all_boxes[idx:idx + num_images, 2:4] = s[:, 0:2]
            all_boxes[idx:idx + num_images, 4] = np.prod(s * 200, 1)  # 沿着axis=1也就是×200后再平方的面积
            all_boxes[idx:idx + num_images, 5] = score
            image_path.extend(meta['image'])  # 将路径信息传回meta

            idx += num_images

            if i % config.PRINT_FREQ == 0:
                msg = 'Test: [{0}/{1}]\t' \
                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t' \
                      'Loss {loss.val:.4f} ({loss.avg:.4f})\t' \
                      'Accuracy {acc.val:.3f} ({acc.avg:.3f})'.format(
                    i, len(val_loader), batch_time=batch_time,
                    loss=losses, acc=acc)
                logger.info(msg)  # len(val_loader)是总的迭代次数

                prefix = '{}_{}'.format(
                    os.path.join(output_dir, 'val'), i
                )  # 'output/mpii/pose_hrnet/w32_256x256_adam_lr1e-3/val_0'
                save_debug_images(config, input, meta, target, pred * 4, output,
                                  prefix)

        name_values, perf_indicator = val_dataset.evaluate(
            config, all_preds, output_dir, all_boxes, image_path,
            filenames, imgnums
        )  # 【】这儿的作用是？

        model_name = config.MODEL.NAME  # 'pose_hrnet'
        if isinstance(name_values, list):
            for name_value in name_values:
                _print_name_value(name_value, model_name)  # 打印相关精度到终端
        else:
            _print_name_value(name_values, model_name)

        if writer_dict:  # 【】None 是不是显示损失用的，怎么用
            writer = writer_dict['writer']
            global_steps = writer_dict['valid_global_steps']
            writer.add_scalar(
                'valid_loss',
                losses.avg,
                global_steps
            )
            writer.add_scalar(
                'valid_acc',
                acc.avg,
                global_steps
            )
            if isinstance(name_values, list):
                for name_value in name_values:
                    writer.add_scalars(
                        'valid',
                        dict(name_value),
                        global_steps
                    )
            else:
                writer.add_scalars(
                    'valid',
                    dict(name_values),
                    global_steps
                )
            writer_dict['valid_global_steps'] = global_steps + 1

    return perf_indicator


# markdown format output
def _print_name_value(name_value, full_arch_name):
    names = name_value.keys()
    values = name_value.values()
    num_values = len(name_value)
    logger.info(
        '| Arch ' +
        ' '.join(['| {}'.format(name) for name in names]) +
        ' |'
    )
    logger.info('|---' * (num_values + 1) + '|')

    if len(full_arch_name) > 15:
        full_arch_name = full_arch_name[:8] + '...'
    logger.info(
        '| ' + full_arch_name + ' ' +
        ' '.join(['| {:.3f}'.format(value) for value in values]) +
        ' |'
    )


class AverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count if self.count != 0 else 0
